{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobo.surrogate_model import BoTorchSurrogateModel\n",
    "from mobo.solver.botroch_solver import qNEHVISolver\n",
    "from mobo.surrogate_problem import SurrogateProblem\n",
    "\n",
    "from problems.common import build_problem\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\queim\\micromambaenv\\envs\\mobo\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.905513763427734\n",
      "43.140201568603516\n",
      "33.97520065307617\n",
      "32.39889907836914\n",
      "30.88536262512207\n",
      "30.37556266784668\n",
      "30.333194732666016\n",
      "30.811912536621094\n",
      "30.840333938598633\n",
      "30.82563591003418\n",
      "30.904239654541016\n",
      "30.56078338623047\n",
      "30.418033599853516\n",
      "30.40366554260254\n",
      "29.947216033935547\n",
      "29.68463134765625\n",
      "29.23786163330078\n",
      "29.205806732177734\n",
      "28.808366775512695\n",
      "28.628103256225586\n",
      "28.46971893310547\n",
      "27.884201049804688\n",
      "27.8964786529541\n",
      "27.303796768188477\n",
      "27.3316650390625\n",
      "26.636533737182617\n",
      "26.283199310302734\n",
      "26.21200180053711\n",
      "25.863445281982422\n",
      "25.856975555419922\n",
      "25.515331268310547\n",
      "25.21506118774414\n",
      "25.08490753173828\n",
      "24.485965728759766\n",
      "24.269908905029297\n",
      "23.99302864074707\n",
      "23.758787155151367\n",
      "23.821086883544922\n",
      "23.71884536743164\n",
      "22.92620277404785\n",
      "22.89940643310547\n",
      "23.036392211914062\n",
      "22.452402114868164\n",
      "22.066085815429688\n",
      "22.594179153442383\n",
      "21.929698944091797\n",
      "21.58176612854004\n",
      "21.861177444458008\n",
      "21.51170539855957\n",
      "21.30780601501465\n",
      "21.10140609741211\n",
      "20.818603515625\n",
      "20.646430015563965\n",
      "20.489727020263672\n",
      "20.306350708007812\n",
      "20.13194751739502\n",
      "19.79932403564453\n",
      "19.647561073303223\n",
      "19.481124877929688\n",
      "19.1751127243042\n",
      "18.547738075256348\n",
      "18.78715705871582\n",
      "18.36189842224121\n",
      "18.442296981811523\n",
      "18.26243495941162\n",
      "17.95781898498535\n",
      "17.75625991821289\n",
      "17.79289436340332\n",
      "17.658554077148438\n",
      "17.322080612182617\n",
      "17.0615873336792\n",
      "16.62471866607666\n",
      "16.619754791259766\n",
      "16.251052856445312\n",
      "15.816678047180176\n",
      "15.69379997253418\n",
      "15.848761558532715\n",
      "15.817512512207031\n",
      "15.579337120056152\n",
      "15.322812557220459\n",
      "15.514158248901367\n",
      "14.879271507263184\n",
      "14.926345348358154\n",
      "15.202522277832031\n",
      "14.462589740753174\n",
      "14.679212093353271\n",
      "14.319337844848633\n",
      "14.273058891296387\n",
      "14.182663917541504\n",
      "13.675799369812012\n",
      "13.929939270019531\n",
      "13.823734760284424\n",
      "13.505492687225342\n",
      "13.114805221557617\n",
      "12.913877010345459\n",
      "12.890085697174072\n",
      "12.684267520904541\n",
      "12.613608598709106\n",
      "12.658217430114746\n",
      "12.264459371566772\n",
      "12.077955722808838\n",
      "11.964055299758911\n",
      "11.559866428375244\n",
      "11.482381880283356\n",
      "11.460346817970276\n",
      "11.446662366390228\n",
      "11.377039968967438\n",
      "11.016667857766151\n",
      "10.728012382984161\n",
      "11.039160788059235\n",
      "10.297455072402954\n",
      "10.182149529457092\n",
      "10.534361124038696\n",
      "10.084532260894775\n",
      "10.544808864593506\n",
      "10.65239942073822\n",
      "9.947589993476868\n",
      "9.9487886428833\n",
      "9.884658336639404\n",
      "9.687300443649292\n",
      "9.350073099136353\n",
      "9.290215015411377\n",
      "9.217180967330933\n",
      "8.73949909210205\n",
      "9.531295776367188\n",
      "8.968791007995605\n",
      "9.026004076004028\n",
      "9.08150339126587\n",
      "8.829214334487915\n",
      "8.999846458435059\n",
      "8.887966632843018\n",
      "8.632482051849365\n",
      "9.235110521316528\n",
      "8.661326885223389\n",
      "8.438126564025879\n",
      "8.834620952606201\n",
      "9.000741004943848\n",
      "8.331336498260498\n",
      "8.20756721496582\n",
      "8.727693557739258\n",
      "8.32777452468872\n",
      "8.191596508026123\n",
      "8.039230823516846\n",
      "7.960200309753418\n",
      "8.263784885406494\n",
      "8.201012134552002\n",
      "8.173375129699707\n",
      "8.105541229248047\n",
      "8.402678489685059\n",
      "7.541330814361572\n",
      "7.66594934463501\n",
      "7.58004903793335\n",
      "7.898718357086182\n",
      "7.57587194442749\n",
      "7.540565490722656\n",
      "7.663560390472412\n",
      "7.486837863922119\n",
      "7.670047760009766\n",
      "7.591351509094238\n",
      "7.720168590545654\n",
      "7.84552001953125\n",
      "8.19645881652832\n",
      "7.742856025695801\n",
      "7.488879680633545\n",
      "7.735964775085449\n",
      "7.814761638641357\n",
      "7.434472560882568\n",
      "7.499192237854004\n",
      "7.782650470733643\n",
      "7.280743598937988\n",
      "7.416240215301514\n",
      "7.3781352043151855\n",
      "7.581099510192871\n",
      "7.170199394226074\n",
      "7.285273551940918\n",
      "7.576823711395264\n",
      "7.010995864868164\n",
      "6.857038497924805\n",
      "7.217195510864258\n",
      "7.058954238891602\n",
      "6.898199558258057\n",
      "6.7719550132751465\n",
      "7.209415435791016\n",
      "6.547842502593994\n",
      "7.173462867736816\n",
      "7.0458598136901855\n",
      "7.069372177124023\n",
      "7.222475528717041\n",
      "7.587222576141357\n",
      "7.781081676483154\n",
      "7.919554710388184\n",
      "7.113112449645996\n",
      "6.881954669952393\n",
      "7.071681976318359\n",
      "6.979766845703125\n",
      "6.773659706115723\n",
      "7.380490303039551\n",
      "7.005646228790283\n",
      "7.197343826293945\n",
      "7.264811992645264\n",
      "6.6287841796875\n",
      "6.8685431480407715\n",
      "6.817928791046143\n",
      "6.77656364440918\n",
      "6.696667194366455\n",
      "7.153284549713135\n",
      "6.350773811340332\n",
      "7.034101963043213\n",
      "6.8756256103515625\n",
      "6.819206714630127\n",
      "6.461160182952881\n",
      "6.764703273773193\n",
      "6.742823600769043\n",
      "6.660130500793457\n",
      "6.677216053009033\n",
      "6.586724281311035\n",
      "6.670765399932861\n",
      "6.967916965484619\n",
      "6.740942478179932\n",
      "6.8098907470703125\n",
      "6.659102916717529\n",
      "6.8395185470581055\n",
      "7.140257835388184\n",
      "6.658767223358154\n",
      "7.245576858520508\n",
      "7.310408592224121\n",
      "6.9240288734436035\n",
      "6.8867268562316895\n",
      "6.657317638397217\n",
      "6.739833831787109\n",
      "6.52163553237915\n",
      "6.680295467376709\n",
      "6.581622123718262\n",
      "6.561240196228027\n",
      "6.819230556488037\n",
      "6.74236536026001\n",
      "6.700474739074707\n",
      "6.699332237243652\n",
      "6.41657829284668\n",
      "6.913963317871094\n",
      "6.498296737670898\n",
      "6.237255096435547\n",
      "6.710955619812012\n",
      "6.373010635375977\n",
      "6.432826042175293\n",
      "6.4797186851501465\n",
      "6.642339706420898\n",
      "6.696601390838623\n",
      "6.693265914916992\n",
      "6.86593770980835\n",
      "6.951142311096191\n",
      "6.626935958862305\n",
      "6.844743251800537\n",
      "6.314643383026123\n",
      "6.63580846786499\n",
      "6.824018955230713\n",
      "6.742766380310059\n",
      "6.328583717346191\n",
      "6.323657035827637\n",
      "6.560636520385742\n",
      "6.513486862182617\n",
      "6.65561580657959\n",
      "6.602889060974121\n",
      "6.431469917297363\n",
      "6.742827415466309\n",
      "6.8784661293029785\n",
      "6.716341972351074\n",
      "6.6511712074279785\n",
      "6.8889241218566895\n",
      "6.36873197555542\n",
      "6.936874866485596\n",
      "6.582230091094971\n",
      "6.320254325866699\n",
      "6.4373698234558105\n",
      "6.385952949523926\n",
      "6.1319169998168945\n",
      "6.203805446624756\n",
      "6.725787162780762\n",
      "6.185967445373535\n",
      "6.5149431228637695\n",
      "6.763402462005615\n",
      "6.320100784301758\n",
      "6.324787616729736\n",
      "6.650230407714844\n",
      "6.5304436683654785\n",
      "6.628010272979736\n",
      "6.575510025024414\n",
      "6.52362060546875\n",
      "6.703110694885254\n",
      "6.649787902832031\n",
      "6.601236820220947\n",
      "6.105249881744385\n",
      "6.256470680236816\n",
      "6.520344257354736\n",
      "6.197721481323242\n",
      "6.10521125793457\n",
      "6.229069709777832\n",
      "6.387199401855469\n",
      "6.416529178619385\n",
      "6.011740684509277\n",
      "6.308196544647217\n",
      "6.601541519165039\n",
      "6.158289432525635\n",
      "6.252622604370117\n",
      "6.202588081359863\n",
      "6.608004570007324\n",
      "6.651788234710693\n",
      "6.931457996368408\n",
      "7.139400005340576\n",
      "7.1541666984558105\n",
      "6.979653835296631\n",
      "6.460848331451416\n",
      "6.318514347076416\n",
      "6.995508193969727\n",
      "6.277839660644531\n",
      "6.408534526824951\n",
      "7.179144859313965\n",
      "6.440573215484619\n",
      "6.31417989730835\n",
      "6.475878715515137\n",
      "6.3873291015625\n",
      "6.179332733154297\n",
      "6.872500419616699\n",
      "6.164893627166748\n",
      "6.162139415740967\n",
      "6.412693977355957\n",
      "6.188688278198242\n",
      "6.3009467124938965\n",
      "6.6061177253723145\n",
      "6.193164348602295\n",
      "6.142876148223877\n",
      "6.248154640197754\n",
      "6.137587547302246\n",
      "6.280401229858398\n",
      "6.310026168823242\n",
      "6.0701518058776855\n",
      "6.321554183959961\n",
      "5.905714988708496\n",
      "6.433698654174805\n",
      "5.844320297241211\n",
      "5.817417621612549\n",
      "6.419990539550781\n",
      "6.107430934906006\n",
      "6.149927139282227\n",
      "6.223822116851807\n",
      "6.2725067138671875\n",
      "5.815110206604004\n",
      "6.089432239532471\n",
      "5.666143417358398\n",
      "6.40276575088501\n",
      "5.767019748687744\n",
      "6.2721076011657715\n",
      "6.046998977661133\n",
      "6.286866664886475\n",
      "6.297446250915527\n",
      "6.298178195953369\n",
      "6.088158130645752\n",
      "6.018245697021484\n",
      "6.094163417816162\n",
      "6.0606255531311035\n",
      "6.148574352264404\n",
      "6.094028472900391\n",
      "6.204820156097412\n",
      "5.903435707092285\n",
      "6.194551944732666\n",
      "6.220795631408691\n",
      "5.996928691864014\n",
      "6.1011962890625\n",
      "5.844879150390625\n",
      "6.158688068389893\n",
      "6.3175272941589355\n",
      "6.010009765625\n",
      "6.0034308433532715\n",
      "6.037415504455566\n",
      "6.088219165802002\n",
      "6.059277057647705\n",
      "6.326436996459961\n",
      "5.972281455993652\n",
      "6.296346187591553\n",
      "6.011335372924805\n",
      "6.115937232971191\n",
      "6.201706886291504\n",
      "5.837225437164307\n",
      "5.870883941650391\n",
      "6.016169548034668\n",
      "5.980765342712402\n",
      "6.0085344314575195\n",
      "5.731647968292236\n",
      "5.790748119354248\n",
      "5.8930182456970215\n",
      "6.306703090667725\n",
      "6.0044264793396\n",
      "5.851284027099609\n",
      "5.835871696472168\n",
      "6.0445709228515625\n",
      "6.032817363739014\n",
      "6.055211067199707\n",
      "6.220931053161621\n",
      "6.470547199249268\n",
      "6.354789733886719\n",
      "6.5040669441223145\n",
      "6.356393814086914\n",
      "6.077576637268066\n",
      "6.061362266540527\n",
      "5.967723369598389\n",
      "6.273489952087402\n",
      "6.450826644897461\n",
      "6.293191909790039\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_x = np.random.uniform(8, 20, 15) \n",
    "train_x_2 = np.random.uniform(8, 20, 15)\n",
    "\n",
    "train_y = 0.1 * train_x ** 2 - 3.5*train_x + np.cos(train_x*1.3) + 40 + np.random.normal(0, 0.01, len(train_x)) * train_x**2 * 0.3\n",
    "train_y_2 = 0.1 * train_x_2 ** 2 - 3.5*train_x_2 + np.cos(train_x_2*1.3) + 40 + np.random.normal(0, 0.01, len(train_x_2)) * train_x_2**2 * 0.3\n",
    "\n",
    "train_x = np.stack([train_x, train_x_2], axis=1)\n",
    "train_y = np.stack([train_y, train_y_2], axis=1)\n",
    "\n",
    "# Create some \"gap\" in the data\n",
    "# to test epistemic uncertainty\n",
    "# estimation:\n",
    "gap_mask = np.array([(train_x[:,0]<12) | (train_x[:,0]>16), (train_x_2<12) | (train_x_2>16)]).T\n",
    "train_x = train_x[gap_mask]\n",
    "train_y = train_y[gap_mask]\n",
    "\n",
    "# Scaling:\n",
    "# train_x = (train_x - train_x.min()) / (train_x.max() - train_x.min())\n",
    "# train_y = (train_y - train_y.mean()) / train_y.std()\n",
    "\n",
    "# Scaling but keep dimensions:\n",
    "train_x = (train_x - train_x.min(axis=0)) / (train_x.max(axis=0) - train_x.min(axis=0))\n",
    "train_y = (train_y - train_y.mean(axis=0)) / train_y.std(axis=0)\n",
    "\n",
    "# Plot training data:\n",
    "plt.plot(train_x, train_y, 'k*')\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import gpytorch\n",
    "\n",
    "class HeteroskedasticGP(gpytorch.models.ApproximateGP):\n",
    "    \n",
    "    def __init__(self, num_inducing=64, name_prefix=\"heteroskedastic_gp\"):\n",
    "        \n",
    "        self.name_prefix = name_prefix\n",
    "\n",
    "        # Define all the variational stuff\n",
    "        inducing_points_task_one = torch.linspace(-1, 1, num_inducing)\n",
    "        inducing_points_task_two = torch.linspace(-1, 1, num_inducing)\n",
    "        \n",
    "        inducing_points = torch.stack([inducing_points_task_one.unsqueeze(1), \n",
    "                                       inducing_points_task_two.unsqueeze(1)])\n",
    "        \n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task (we have 2):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=inducing_points.size(-2), \n",
    "            batch_shape=torch.Size([2]))\n",
    "        \n",
    "        single_variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        \n",
    "        # Wrap the single variational strategy into a\n",
    "        # Linear Model of Coregionalization one, so the two\n",
    "        # tasks (and therefore latent GPs) are assumed to be\n",
    "        # somehow related (and we learn such relationship):\n",
    "        variational_strategy = gpytorch.variational.LMCVariationalStrategy(single_variational_strategy,\n",
    "                                                                           num_tasks=2,\n",
    "                                                                           num_latents=2)\n",
    "        # Standard initializtation\n",
    "        super().__init__(variational_strategy)\n",
    "        \n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters:\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([2]))\n",
    "        \n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([2])),\n",
    "            batch_shape=torch.Size([2])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def guide(self, x, y):\n",
    "        \n",
    "        # Get q(f) - variational (guide) distribution of latent function\n",
    "        function_dist = self.pyro_guide(x)\n",
    "\n",
    "        # Use a plate here to mark conditional independencies.\n",
    "        # Our samples are of shape (nobs, 2), therefore the dim=-2\n",
    "        # in the plate:\n",
    "        with pyro.plate(self.name_prefix + \".data_plate\", dim=-2):\n",
    "            \n",
    "            # Sample from latent function distribution\n",
    "            function_samples = pyro.sample(self.name_prefix + \".f(x)\", function_dist)\n",
    "\n",
    "    def model(self, x, y):\n",
    "        \n",
    "        pyro.module(self.name_prefix + \".gp\", self)\n",
    "\n",
    "        # Get p(f) - prior distribution of latent function\n",
    "        function_dist = self.pyro_model(x)\n",
    "\n",
    "        # Use a plate here to mark conditional independencies.\n",
    "        # Our samples are of shape (nobs, 2), therefore the dim=-2\n",
    "        # in the plate:\n",
    "        with pyro.plate(self.name_prefix + \".data_plate\", dim=-2):\n",
    "            \n",
    "            # Sample from latent function distribution\n",
    "            function_samples = pyro.sample(self.name_prefix + \".f(x)\", function_dist)\n",
    "            \n",
    "            mean_samples = function_samples[...,0]\n",
    "            std_samples = function_samples[...,1]\n",
    "            \n",
    "            # Exp to force always nonnegative stddevs:\n",
    "            transformed_std_samples = torch.exp(std_samples)\n",
    "\n",
    "            # Sample from observed distribution\n",
    "            return pyro.sample(self.name_prefix + \".y\",\n",
    "                               pyro.distributions.Normal(mean_samples, transformed_std_samples),\n",
    "                               obs=y)\n",
    "\n",
    "# Instantiate model:\n",
    "pyro.clear_param_store()  # Good practice\n",
    "model = HeteroskedasticGP()\n",
    "\n",
    "num_iter = 1000\n",
    "num_particles = 256\n",
    "\n",
    "# training routine:\n",
    "def train():\n",
    "    optimizer = pyro.optim.Adam({\"lr\": 0.01})\n",
    "    elbo = pyro.infer.Trace_ELBO(num_particles=num_particles, vectorize_particles=True, retain_graph=True)\n",
    "    svi = pyro.infer.SVI(model.model, model.guide, optimizer, elbo)\n",
    "\n",
    "    model.train()\n",
    "    iterator = range(num_iter)\n",
    "    for i in iterator:\n",
    "        model.zero_grad()\n",
    "        loss = svi.step(torch.from_numpy(train_x).float().unsqueeze(1), torch.from_numpy(train_y).float())\n",
    "        print(loss)\n",
    "\n",
    "# Train the GP:\n",
    "train()\n",
    "\n",
    "# Function to plot predictions:\n",
    "def plot_preds(ax, feature, mean_samples, var_samples, bootstrap=True, n_boots=100,\n",
    "               show_epistemic=False, epistemic_mean=None, epistemic_var=None):\n",
    "    \"\"\"Plots the overall mean and variance of the aggregate system.\n",
    "    Inherited from https://www.pymc.io/projects/examples/en/latest/gaussian_processes/GP-Heteroskedastic.html.\n",
    "\n",
    "    We can represent the overall uncertainty via explicitly sampling the underlying normal\n",
    "    distributrions (with `bootstrap=True`) or as the mean +/- the standard deviation from\n",
    "    the Law of Total Variance. \n",
    "    \n",
    "    For systems with many observations, there will likely be\n",
    "    little difference, but in cases with few observations and informative priors, plotting\n",
    "    the percentiles will likely give a more accurate representation.\n",
    "    \"\"\"\n",
    "    if bootstrap:\n",
    "        means = np.expand_dims(mean_samples.T, axis=2)\n",
    "        stds = np.sqrt(np.expand_dims(var_samples.T, axis=2))\n",
    "        \n",
    "        samples_shape = mean_samples.T.shape + (n_boots,)\n",
    "        samples = np.random.normal(means, stds, samples_shape)\n",
    "        \n",
    "        reshaped_samples = samples.reshape(mean_samples.shape[1], -1).T\n",
    "        \n",
    "        l, m, u = [np.percentile(reshaped_samples, p, axis=0) for p in [2.5, 50, 97.5]]\n",
    "        ax.plot(feature, m, label=\"Median\", color=\"b\")\n",
    "    \n",
    "    else:\n",
    "        m = mean_samples.mean(axis=0)\n",
    "        sd = np.sqrt(mean_samples.var(axis=0) + var_samples.mean(axis=0))\n",
    "        l, u = m - 1.96 * sd, m + 1.96 * sd\n",
    "        ax.plot(feature, m, label=\"Mean\", color=\"b\")\n",
    "    \n",
    "    if show_epistemic:\n",
    "        ax.fill_between(feature,\n",
    "                        l,\n",
    "                        epistemic_mean-1.96*np.sqrt(epistemic_var), \n",
    "                        alpha=0.2,\n",
    "                        color=\"#88b4d2\",\n",
    "                        label=\"Total Uncertainty (95%)\")\n",
    "        ax.fill_between(feature, \n",
    "                        u, \n",
    "                        epistemic_mean+1.96*np.sqrt(epistemic_var), \n",
    "                        alpha=0.2,\n",
    "                        color=\"#88b4d2\")\n",
    "        ax.fill_between(feature, \n",
    "                        epistemic_mean-1.96*np.sqrt(epistemic_var), \n",
    "                        epistemic_mean+1.96*np.sqrt(epistemic_var),\n",
    "                        alpha=0.4,\n",
    "                        color=\"#88b4d2\",\n",
    "                        label=\"Epistemic Uncertainty (95%)\")\n",
    "    else:\n",
    "        ax.fill_between(feature, \n",
    "                        l, \n",
    "                        u, \n",
    "                        alpha=0.2,\n",
    "                        color=\"#88b4d2\",\n",
    "                        label=\"Total Uncertainty (95%)\")\n",
    "\n",
    "# Test data to predict:\n",
    "x_padding = 0.1\n",
    "\n",
    "test_x = torch.linspace(train_x.min() - (train_x.max() - train_x.min()) * x_padding, \n",
    "                        train_x.max() + (train_x.max() - train_x.min()) * x_padding, \n",
    "                        100).float()\n",
    "\n",
    "# Predict:\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_dist = model(test_x)\n",
    "\n",
    "# Extract predictions:\n",
    "output_samples = output_dist.sample(torch.Size([1000]))\n",
    "mu_samples = output_samples[...,0]\n",
    "sigma_samples = torch.exp(output_samples[...,1])\n",
    "\n",
    "# Plot predictions:\n",
    "plt.plot(train_x, train_y, 'k*', label=\"Observed Data\")\n",
    "ax = plt.gca()\n",
    "plot_preds(ax, \n",
    "           test_x.numpy(),\n",
    "           mu_samples.numpy(), \n",
    "           sigma_samples.numpy()**2,\n",
    "           bootstrap=False, \n",
    "           n_boots=100,\n",
    "           show_epistemic=True,\n",
    "           epistemic_mean=output_dist.mean[:,0].detach().numpy(),\n",
    "           epistemic_var=output_dist.stddev[:,0].detach().numpy()**2)\n",
    "ax.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_model(train_x, train_y, train_rho=None):\n",
    "    # define models for objective and constraint\n",
    "    train_y_mean = -train_y  # negative because botorch assumes maximization\n",
    "    # train_y_var = self.real_problem.evaluate(train_x).to(**tkwargs).var(dim=-1)\n",
    "    train_y_var = train_rho + 1e-6\n",
    "    models = []\n",
    "    for i in range(train_y.shape[1]):\n",
    "        train_y_i = train_y_mean[..., i]\n",
    "        train_yvar_i = train_y_var[..., i]\n",
    "        # models.append(\n",
    "        #     FixedNoiseGP(\n",
    "        #         train_X=train_x,\n",
    "        #         train_Y=train_y_i.unsqueeze(-1),\n",
    "        #         train_Yvar=train_yvar_i.unsqueeze(-1),\n",
    "        #         outcome_transform=Standardize(m=1),\n",
    "        #     )\n",
    "        # )\n",
    "        models.append(\n",
    "            HeteroskedasticSingleTaskGP(\n",
    "                train_X=train_x,\n",
    "                train_Y=train_y_i.unsqueeze(-1),\n",
    "                train_Yvar=train_yvar_i.unsqueeze(-1),\n",
    "                outcome_transform=Standardize(m=1),\n",
    "            )\n",
    "        )\n",
    "        # models.append(\n",
    "        #     SingleTaskGP(\n",
    "        #         train_x,\n",
    "        #         train_y_i.unsqueeze(-1),\n",
    "        #         # train_yvar_i.unsqueeze(-1),\n",
    "        #         outcome_transform=Standardize(m=1),\n",
    "        #     )\n",
    "        # )\n",
    "    model = ModelListGP(*models)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
